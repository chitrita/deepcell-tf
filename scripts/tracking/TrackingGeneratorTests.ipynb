{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import errno\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.optimizers import SGD, Adam\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "\n",
    "from deepcell import get_data\n",
    "#from deepcell import make_training_data\n",
    "#from deepcell import rate_scheduler\n",
    "#from deepcell.model_zoo import siamese_model\n",
    "#from deepcell.training import train_model_siamese_daughter\n",
    "#from deepcell.image_generators import MovieDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements for image_generators.py\n",
    "from keras_preprocessing.image import Iterator, ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import special_ortho_group\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "# Custom Siamese Generators\n",
    "class SiameseDataGenerator(ImageDataGenerator):\n",
    "    def flow(self,\n",
    "             train_dict,\n",
    "             crop_dim=32,\n",
    "             min_track_length=5,\n",
    "             features=None,\n",
    "             sync_transform=True,\n",
    "             batch_size=32,\n",
    "             shuffle=True,\n",
    "             seed=None,\n",
    "             data_format=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png'):\n",
    "        return SiameseIterator(\n",
    "            train_dict,\n",
    "            self,\n",
    "            crop_dim=crop_dim,\n",
    "            min_track_length=min_track_length,\n",
    "            features=features,\n",
    "            sync_transform=sync_transform,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            data_format=data_format,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format)\n",
    "\n",
    "\n",
    "class SiameseIterator(Iterator):\n",
    "    def __init__(self,\n",
    "                 train_dict,\n",
    "                 image_data_generator,\n",
    "                 crop_dim=14,\n",
    "                 min_track_length=5,\n",
    "                 batch_size=32,\n",
    "                 occupancy_grid_size=10,\n",
    "                 occupancy_window=100,\n",
    "                 features=None,\n",
    "                 sync_transform=True,\n",
    "                 shuffle=False,\n",
    "                 seed=None,\n",
    "                 squeeze=False,\n",
    "                 data_format=None,\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png'):\n",
    "        if data_format is None:\n",
    "            data_format = K.image_data_format()\n",
    "\n",
    "        if data_format == 'channels_first':\n",
    "            self.channel_axis = 1\n",
    "            self.row_axis = 3\n",
    "            self.col_axis = 4\n",
    "            self.time_axis = 2\n",
    "        if data_format == 'channels_last':\n",
    "            self.channel_axis = 4\n",
    "            self.row_axis = 2\n",
    "            self.col_axis = 3\n",
    "            self.time_axis = 1\n",
    "            \n",
    "        if features is None:\n",
    "            raise ValueError(\"SiameseIterator: No features specified.\")\n",
    "\n",
    "        self.x = np.asarray(train_dict['X'], dtype=K.floatx())\n",
    "        self.y = np.array(train_dict['y'], dtype='int32')\n",
    "\n",
    "        if self.x.ndim != 5:\n",
    "            raise ValueError('Input data in `SiameseIterator` '\n",
    "                             'should have rank 5. You passed an array '\n",
    "                             'with shape', self.x.shape)\n",
    "\n",
    "        self.crop_dim = crop_dim\n",
    "        self.min_track_length = min_track_length\n",
    "        self.features = sorted(features)\n",
    "        self.sync_transform = sync_transform\n",
    "        self.occupancy_grid_size = np.int(occupancy_grid_size)\n",
    "        self.occupancy_window = np.int(occupancy_window)\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.squeeze = squeeze\n",
    "        self.data_format = data_format\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "\n",
    "        if 'daughters' in train_dict:\n",
    "            self.daughters = train_dict['daughters']\n",
    "        else:\n",
    "            self.daughters = None\n",
    "\n",
    "        self._remove_bad_images()\n",
    "        self._create_track_ids()\n",
    "        self._create_features()\n",
    "\n",
    "        super(SiameseIterator, self).__init__(\n",
    "            len(self.track_ids), batch_size, shuffle, seed)\n",
    "\n",
    "    def _remove_bad_images(self):\n",
    "        \"\"\"\n",
    "        This function goes through all of the batches of images and removes the \n",
    "        images that only have one cell.\n",
    "        \"\"\"\n",
    "        good_batches = []\n",
    "        number_of_batches = self.x.shape[0]\n",
    "        for batch in range(number_of_batches):\n",
    "            y = self.y[batch]\n",
    "            unique_ids = np.unique(y.flatten())\n",
    "            if len(unique_ids) > 2: # You should have at least 3 id's - 2 cells and 1 background\n",
    "                good_batches.append(batch)\n",
    "\n",
    "        X_new_shape = (len(good_batches), *self.x.shape[1:])\n",
    "        y_new_shape = (len(good_batches), *self.y.shape[1:])\n",
    "\n",
    "        X_new = np.zeros(X_new_shape, dtype = K.floatx())\n",
    "        y_new = np.zeros(y_new_shape, dtype = np.int32)\n",
    "\n",
    "        counter = 0\n",
    "        for k, batch in enumerate(good_batches):\n",
    "            X_new[k] = self.x[batch]\n",
    "            y_new[k] = self.y[batch]\n",
    "\n",
    "        self.x = X_new\n",
    "        self.y = y_new\n",
    "\n",
    "    def _create_track_ids(self):\n",
    "        \"\"\"\n",
    "        This function builds the track id's. It returns a dictionary that\n",
    "        contains the batch number and label number of each each track.\n",
    "        Creates unique cell IDs, as cell labels are NOT unique across batches.\n",
    "        \"\"\"\n",
    "        track_counter = 0\n",
    "        track_ids = {}\n",
    "        for batch in range(self.y.shape[0]):\n",
    "            y_batch = self.y[batch]\n",
    "            num_cells = np.amax(y_batch)\n",
    "            for cell in range(1, num_cells + 1):\n",
    "                # count number of pixels cell occupies in each frame\n",
    "                y_true = np.sum(y_batch == cell, axis=(self.row_axis - 1, self.col_axis - 1))\n",
    "                # get indices of frames where cell is present\n",
    "                y_index = np.where(y_true > 0)[0]\n",
    "                if y_index.size > 3: #self.min_track_length+1:  # if cell is present at all\n",
    "                    if self.daughters is not None:\n",
    "                        # Only include daughters if there are enough frames in their tracks\n",
    "                        try:\n",
    "                            daughter_ids = self.daughters[batch][cell]\n",
    "                        except:\n",
    "                            print('batch', batch)\n",
    "                            print('cell', cell)\n",
    "                            print('daughter shape', self.daughters.shape)\n",
    "                            print('batch daughter shape', self.daughters[batch].shape)\n",
    "                            print('num_cells', num_cells)\n",
    "                            print('max num of cells', np.amax(y_batch))\n",
    "                        if len(daughter_ids) > 0:\n",
    "                            daughter_track_lengths = []\n",
    "                            for did in daughter_ids:\n",
    "                                # Screen daughter tracks to make sure they are long enough\n",
    "                                # Length currently set to 0\n",
    "                                d_true = np.sum(y_batch == did, axis=(self.row_axis - 1, self.col_axis - 1))\n",
    "                                d_track_length = len(np.where(d_true>0)[0])\n",
    "                                daughter_track_lengths.append(d_track_length > 3)\n",
    "                            keep_daughters = all(daughter_track_lengths)\n",
    "                            daughters = daughter_ids if keep_daughters else []\n",
    "                        else:\n",
    "                            daughters = []\n",
    "                    else:\n",
    "                        daughters = []\n",
    "                            \n",
    "                    track_ids[track_counter] = {\n",
    "                        'batch': batch,\n",
    "                        'label': cell,\n",
    "                        'frames': y_index,\n",
    "                        'daughters': daughters  \n",
    "                    }\n",
    "                                \n",
    "                    track_counter += 1\n",
    "\n",
    "                else:\n",
    "                    y_batch[y_batch == cell] = 0\n",
    "                    self.y[batch] = y_batch\n",
    "                    \n",
    "        # Add a field to the track_ids dict that locates all of the different cells\n",
    "        # in each frame\n",
    "        for track in track_ids.keys():\n",
    "            track_ids[track]['different'] = {}\n",
    "            batch = track_ids[track]['batch']\n",
    "            label = track_ids[track]['label']\n",
    "            for frame in track_ids[track]['frames']:\n",
    "                y_unique = np.unique(self.y[batch][frame])\n",
    "                y_unique = np.delete(y_unique, np.where(y_unique == 0))\n",
    "                y_unique = np.delete(y_unique, np.where(y_unique == label))\n",
    "                track_ids[track]['different'][frame] = y_unique  \n",
    "                        \n",
    "        # We will need to look up the track_ids of cells if we know their batch and label. We will \n",
    "        # create a dictionary that stores this information\n",
    "        reverse_track_ids = {}\n",
    "        for batch in range(self.y.shape[0]):\n",
    "            reverse_track_ids[batch] = {}\n",
    "        for track in track_ids.keys():\n",
    "            batch = track_ids[track]['batch']\n",
    "            label = track_ids[track]['label']\n",
    "            reverse_track_ids[batch][label] = track\n",
    "            \n",
    "        # Save dictionaries\n",
    "        self.track_ids = track_ids\n",
    "        self.reverse_track_ids = reverse_track_ids\n",
    "        \n",
    "        # Identify which tracks have divisions\n",
    "        self.tracks_with_divisions = []\n",
    "        for track in self.track_ids.keys():\n",
    "            if len(self.track_ids[track]['daughters']) > 0:\n",
    "                self.tracks_with_divisions.append(track)\n",
    "\n",
    "    def _get_features(self, X, y, frames, labels):\n",
    "        channel_axis = self.channel_axis - 1\n",
    "        if self.data_format == 'channels_first':\n",
    "            appearance_shape = (X.shape[channel_axis],\n",
    "                                len(frames),\n",
    "                                self.crop_dim,\n",
    "                                self.crop_dim)\n",
    "        else:\n",
    "            appearance_shape = (len(frames),\n",
    "                                self.crop_dim,\n",
    "                                self.crop_dim,\n",
    "                                X.shape[channel_axis])\n",
    "\n",
    "        occupancy_grid_shape = (len(frames), 2*self.occupancy_grid_size+1, 2*self.occupancy_grid_size+1,1)\n",
    "        \n",
    "        # Initialize storage for appearances and centroids\n",
    "        appearances = np.zeros(appearance_shape, dtype=K.floatx())\n",
    "        centroids = []\n",
    "        perimeters = []\n",
    "        occupancy_grids = np.zeros(occupancy_grid_shape, dtype = K.floatx())\n",
    "\n",
    "        for counter, (frame, cell_label) in enumerate(zip(frames, labels)):\n",
    "            # Get the bounding box\n",
    "            X_frame = X[frame] if self.data_format == 'channels_last' else X[:, frame]\n",
    "            y_frame = y[frame] if self.data_format == 'channels_last' else y[:, frame]\n",
    "            props = regionprops(np.int32(y_frame == cell_label))\n",
    "            minr, minc, maxr, maxc = props[0].bbox\n",
    "            centroids.append(props[0].centroid)\n",
    "            perimeters.append(np.array([props[0].perimeter]))\n",
    "\n",
    "            # Extract images from bounding boxes\n",
    "            if self.data_format == 'channels_first':\n",
    "                appearance = np.copy(X[:, frame, minr:maxr, minc:maxc])\n",
    "                resize_shape = (X.shape[channel_axis], self.crop_dim, self.crop_dim)\n",
    "            else:\n",
    "                appearance = np.copy(X[frame, minr:maxr, minc:maxc, :])\n",
    "                resize_shape = (self.crop_dim, self.crop_dim, X.shape[channel_axis])\n",
    "\n",
    "            # Resize images from bounding box\n",
    "            max_value = np.amax([np.amax(appearance), np.absolute(np.amin(appearance))])\n",
    "            appearance /= max_value\n",
    "            appearance = resize(appearance, resize_shape, mode='constant')\n",
    "            appearance *= max_value\n",
    "            if self.data_format == 'channels_first':\n",
    "                appearances[:, counter] = appearance\n",
    "            else:\n",
    "                appearances[counter] = appearance\n",
    "                \n",
    "            # Get occupancy grid\n",
    "            occupancy_grid = np.zeros((2*self.occupancy_grid_size+1, 2*self.occupancy_grid_size+1,1), \n",
    "                                      dtype=K.floatx())\n",
    "            X_padded = np.pad(X_frame, ((self.occupancy_window, self.occupancy_window), \n",
    "                                        (self.occupancy_window, self.occupancy_window),\n",
    "                                        (0,0)), mode='constant', constant_values=0)\n",
    "            y_padded = np.pad(y_frame, ((self.occupancy_window, self.occupancy_window), \n",
    "                                        (self.occupancy_window, self.occupancy_window),\n",
    "                                        (0,0)), mode='constant', constant_values=0)\n",
    "            props = regionprops(np.int32(y_padded == cell_label))\n",
    "            center_x, center_y = props[0].centroid\n",
    "            center_x, center_y = np.int(center_x), np.int(center_y)\n",
    "            X_reduced = X_padded[center_x-self.occupancy_window:center_x+self.occupancy_window,\n",
    "                                 center_y-self.occupancy_window:center_y+self.occupancy_window,:]\n",
    "            y_reduced = y_padded[center_x-self.occupancy_window:center_x+self.occupancy_window,\n",
    "                                 center_y-self.occupancy_window:center_y+self.occupancy_window,:]\n",
    "            \n",
    "            # Resize X_reduced in case it is used instead of the occupancy grid method\n",
    "            resize_shape = (2*self.occupancy_grid_size+1, 2*self.occupancy_grid_size+1, X.shape[channel_axis])\n",
    "\n",
    "            # Resize images from bounding box\n",
    "            max_value = np.amax([np.amax(X_reduced), np.absolute(np.amin(X_reduced))])\n",
    "            X_reduced /= max_value\n",
    "            X_reduced = resize(X_reduced, resize_shape, mode='constant')\n",
    "            X_reduced *= max_value\n",
    "                    \n",
    "            occupancy_grids[counter,:,:,:] = X_reduced #occupancy_grid\n",
    "            \n",
    "        return [appearances, centroids, occupancy_grids, perimeters]\n",
    "\n",
    "    def _create_features(self):\n",
    "        \"\"\"\n",
    "        This function gets the features of every cell, crops them out, resizes them, \n",
    "        and stores them in an matrix. Pre-fetching the features should significantly \n",
    "        speed up the generator. It also gets the centroids and occupancy grids\n",
    "        \"\"\"\n",
    "        number_of_tracks = len(self.track_ids.keys())\n",
    "\n",
    "        # Initialize the array for the appearances and centroids\n",
    "        if self.data_format =='channels_first':\n",
    "            all_appearances_shape = (number_of_tracks, self.x.shape[self.channel_axis], self.x.shape[self.time_axis], self.crop_dim, self.crop_dim)\n",
    "        if self.data_format == 'channels_last':\n",
    "            all_appearances_shape = (number_of_tracks, self.x.shape[self.time_axis], self.crop_dim, self.crop_dim, self.x.shape[self.channel_axis])\n",
    "        all_appearances = np.zeros(all_appearances_shape, dtype=K.floatx())\n",
    "\n",
    "        all_centroids_shape = (number_of_tracks, self.x.shape[self.time_axis], 2)\n",
    "        all_centroids = np.zeros(all_centroids_shape, dtype=K.floatx())\n",
    "        \n",
    "        all_occupancy_grids_shape = (number_of_tracks, self.x.shape[self.time_axis], \n",
    "                                     2 * self.occupancy_grid_size + 1, 2 * self.occupancy_grid_size + 1, 1)\n",
    "        all_occupancy_grids = np.zeros(all_occupancy_grids_shape, dtype=K.floatx())\n",
    "        \n",
    "        all_perimeters_shape = (number_of_tracks, self.x.shape[self.time_axis], 1)\n",
    "        all_perimeters = np.zeros(all_perimeters_shape, dtype=K.floatx())\n",
    "\n",
    "        for track in self.track_ids.keys():\n",
    "            batch = self.track_ids[track]['batch']\n",
    "            label = self.track_ids[track]['label']\n",
    "            frames = self.track_ids[track]['frames']\n",
    "            # frames = np.append(frames, frames[-1])\n",
    "\n",
    "            # Make an array of labels that the same length as the frames array\n",
    "            labels = [label] * len(frames)\n",
    "            X = self.x[batch]\n",
    "            y = self.y[batch]\n",
    "\n",
    "            appearance, centroid, occupancy_grid, perimeter = self._get_features(X, y, frames, labels)\n",
    "\n",
    "            if self.data_format == 'channels_first':\n",
    "                all_appearances[track,:,np.array(frames),:,:] = appearance \n",
    "            if self.data_format == 'channels_last':\n",
    "                all_appearances[track,np.array(frames),:,:,:] = appearance\n",
    "\n",
    "            all_centroids[track, np.array(frames),:] = centroid\n",
    "            all_occupancy_grids[track, np.array(frames),:,:] = occupancy_grid\n",
    "            all_perimeters[track, np.array(frames),:] = perimeter\n",
    "            \n",
    "        self.all_appearances = all_appearances\n",
    "        self.all_centroids = all_centroids\n",
    "        self.all_occupancy_grids = all_occupancy_grids\n",
    "        self.all_perimeters = all_perimeters\n",
    "\n",
    "    def _fetch_appearances(self, track, frames):\n",
    "        \"\"\"\n",
    "        This function gets the appearances after they have been \n",
    "        cropped out of the image\n",
    "        \"\"\"\n",
    "        # TO DO: Check to make sure the frames are acceptable\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            appearances = self.all_appearances[track,:,np.array(frames),:,:]\n",
    "        if self.data_format == 'channels_last':\n",
    "            appearances = self.all_appearances[track,np.array(frames),:,:,:]\n",
    "        return appearances\n",
    "\n",
    "    def _fetch_centroids(self, track, frames):\n",
    "        \"\"\"\n",
    "        This function gets the centroids after they have been\n",
    "        extracted and stored\n",
    "        \"\"\"\n",
    "        # TO DO: Check to make sure the frames are acceptable\n",
    "        return self.all_centroids[track,np.array(frames),:]\n",
    "    \n",
    "    def _fetch_occupancy_grids(self, track, frames):\n",
    "        \"\"\"\n",
    "        This function gets the occupancy grids after they have been\n",
    "        extracted and stored\n",
    "        \"\"\"\n",
    "        # TO DO: Check to make sure the frames are acceptable\n",
    "        return self.all_occupancy_grids[track,np.array(frames),:,:,:]\n",
    "    \n",
    "    def _fetch_perimeters(self, track, frames):\n",
    "        \"\"\"\n",
    "        This function gets the centroids after they have been\n",
    "        extracted and stored\n",
    "        \"\"\"\n",
    "        # TO DO: Check to make sure the frames are acceptable\n",
    "        return self.all_perimeters[track,np.array(frames)]\n",
    "\n",
    "    def _fetch_frames(self, track, division=False):\n",
    "        \"\"\"\n",
    "        This function fetches a random list of frames for a given track.\n",
    "        If the division flag is true, then the list of frames ends at the cell's\n",
    "        last appearance if the division flag is true.\n",
    "        \"\"\"\n",
    "        track_id = self.track_ids[track]\n",
    "        batch = track_id['batch']\n",
    "        tracked_frames = list(track_id['frames'])\n",
    "\n",
    "        # We need to have at least one future frame to pick from, so if \n",
    "        # the last frame of the movie is a tracked frame, remove it\n",
    "        last_frame = self.x.shape[self.time_axis] - 1\n",
    "        if last_frame in tracked_frames:\n",
    "            tracked_frames.remove(last_frame)\n",
    "\n",
    "        # Get the indices of the tracked_frames list - sometimes frames\n",
    "        # are skipped\n",
    "        tracked_frames_index = np.arange(len(tracked_frames))\n",
    "        \n",
    "        # Check if there are enough frames\n",
    "        enough_frames = len(tracked_frames_index) > self.min_track_length + 1\n",
    "\n",
    "        # We need to exclude the last frame so that we will always be able to make a comparison\n",
    "        acceptable_indices = tracked_frames_index[self.min_track_length-1:-1] if enough_frames else tracked_frames_index[:-1]\n",
    "\n",
    "        # Take the last frame if there is a division, otherwise randomly pick a frame\n",
    "        index = -1 if division else np.random.choice(acceptable_indices) \n",
    "\n",
    "        # Select the frames. If there aren't enough frames, repeat the first frame\n",
    "        # the necessary number of times\n",
    "        if enough_frames:\n",
    "            frames = tracked_frames[index+1-self.min_track_length:index+1]\n",
    "        else:\n",
    "            frames_temp = tracked_frames[0:index+1]\n",
    "            missing_frames = self.min_track_length - len(frames_temp)\n",
    "            frames = [tracked_frames[0]] * missing_frames + frames_temp\n",
    "\n",
    "        return frames\n",
    "    \n",
    "    def _compute_appearances(self, track_1, frames_1, track_2, frames_2, transform):\n",
    "        appearance_1 = self._fetch_appearances(track_1, frames_1)\n",
    "        appearance_2 = self._fetch_appearances(track_2, frames_2)\n",
    "            \n",
    "        # Apply random transforms\n",
    "        new_appearance_1 = np.zeros(appearance_1.shape, dtype=K.floatx())\n",
    "        new_appearance_2 = np.zeros(appearance_2.shape, dtype=K.floatx())\n",
    "\n",
    "        for frame in range(appearance_1.shape[self.time_axis-1]):\n",
    "            if self.data_format == 'channels_first':\n",
    "                if transform is not None:\n",
    "                    app_temp = self.image_data_generator.apply_transform(appearance_1[:,frame,:,:], transform)\n",
    "                else:\n",
    "                    app_temp = self.image_data_generator.random_transform(appearance_1[:,frame,:,:])\n",
    "                app_temp = self.image_data_generator.standardize(app_temp)\n",
    "                new_appearance_1[:,frame,:,:] = app_temp                        \n",
    "\n",
    "            if self.data_format == 'channels_last':\n",
    "                if transform is not None:\n",
    "                    app_temp = self.image_data_generator.apply_transform(appearance_1[frame], transform)\n",
    "                else:\n",
    "                    self.image_data_generator.random_transform(appearance_1[frame])\n",
    "                app_temp = self.image_data_generator.standardize(app_temp)\n",
    "                new_appearance_1[frame] = app_temp\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            if transform is not None:\n",
    "                app_temp = self.image_data_generator.apply_transform(appearance_2[:,0,:,:], transform)\n",
    "            else:\n",
    "                app_temp = self.image_data_generator.random_transform(appearance_2[:,0,:,:])\n",
    "            app_temp = self.image_data_generator.standardize(app_temp)\n",
    "            new_appearance_2[:,0,:,:] = app_temp   \n",
    "\n",
    "        if self.data_format == 'channels_last':\n",
    "            if transform is not None:\n",
    "                app_temp = self.image_data_generator.apply_transform(appearance_2[0], transform)\n",
    "            else:\n",
    "                app_temp = self.image_data_generator.random_transform(appearance_2[0])\n",
    "            app_temp = self.image_data_generator.standardize(app_temp)\n",
    "            new_appearance_2[0] = app_temp\n",
    "\n",
    "        return new_appearance_1, new_appearance_2\n",
    "\n",
    "    def _compute_distances(self, track_1, frames_1, track_2, frames_2, transform):\n",
    "        centroid_1 = self._fetch_centroids(track_1, frames_1)\n",
    "        centroid_2 = self._fetch_centroids(track_2, frames_2)\n",
    "        \n",
    "        # Compute distances between centroids\n",
    "        centroids = np.concatenate([centroid_1, centroid_2], axis=0)\n",
    "        distance = np.diff(centroids, axis=0)\n",
    "        zero_pad = np.zeros((1, 2), dtype=K.floatx())\n",
    "        distance = np.concatenate([zero_pad, distance], axis=0)\n",
    "\n",
    "        distance_1 = distance[0:-1,:]\n",
    "        distance_2 = distance[-1,:]\n",
    "            \n",
    "        return distance_1, distance_2\n",
    "\n",
    "    def _compute_perimeters(self, track_1, frames_1, track_2, frames_2, transform):\n",
    "        perimeter_1 = self._fetch_perimeters(track_1, frames_1)\n",
    "        perimeter_2 = self._fetch_perimeters(track_2, frames_2)\n",
    "        \n",
    "        return perimeter_1, perimeter_2\n",
    "    \n",
    "    def _compute_occupancy_grids(self, track_1, frames_1, track_2, frames_2, transform):\n",
    "        occupancy_grid_1 = self._fetch_occupancy_grids(track_1, frames_1)\n",
    "        occupancy_grid_2 = self._fetch_occupancy_grids(track_2, frames_2)\n",
    "\n",
    "        # Randomly transform the occupancy maps\n",
    "        occupancy_generator = ImageDataGenerator(rotation_range=180, \n",
    "                                                 horizontal_flip=True,\n",
    "                                                 vertical_flip=True)\n",
    "\n",
    "        occupancy_grids = np.concatenate([occupancy_grid_1, occupancy_grid_2], axis=0)\n",
    "        \n",
    "        for frame in range(occupancy_grids.shape[self.time_axis-1]):\n",
    "            og_temp = occupancy_grids[frame]\n",
    "            if transform is not None:\n",
    "                og_temp = self.image_data_generator.apply_transform(og_temp, transform)\n",
    "            else:\n",
    "                og_temp = self.image_data_generator.random_transform(og_temp)\n",
    "            occupancy_grids[frame] = og_temp\n",
    "\n",
    "        occupancy_grid_1 = occupancy_grids[0:-1,:,:,:]\n",
    "        occupancy_grid_2 = occupancy_grids[-1,:,:,:]\n",
    "\n",
    "        return occupancy_grid_1, occupancy_grid_2\n",
    "    \n",
    "    def _compute_feature_shape(self, feature, index_array):\n",
    "        if feature == \"appearance\":\n",
    "            if self.data_format == 'channels_first':\n",
    "                shape_1 = (len(index_array), self.x.shape[self.channel_axis],\n",
    "                            self.min_track_length, self.crop_dim, self.crop_dim)\n",
    "                shape_2 = (len(index_array), self.x.shape[self.channel_axis],\n",
    "                            self.crop_dim, self.crop_dim)\n",
    "            else:\n",
    "                shape_1 = (len(index_array), self.min_track_length,self.crop_dim, self.crop_dim,\n",
    "                            self.x.shape[self.channel_axis])\n",
    "                shape_2 = (len(index_array), 1, self.crop_dim, self.crop_dim,\n",
    "                            self.x.shape[self.channel_axis])\n",
    "\n",
    "        elif feature == \"distance\":\n",
    "            shape_1 = (len(index_array), self.min_track_length, 2)\n",
    "            shape_2 = (len(index_array), 1, 2)\n",
    "            \n",
    "        elif feature == \"neighborhood\":        \n",
    "            shape_1 = (len(index_array), self.min_track_length, \n",
    "                       2 * self.occupancy_grid_size + 1, 2 * self.occupancy_grid_size + 1, 1)\n",
    "            shape_2 = (len(index_array), 1, 2 * self.occupancy_grid_size + 1,\n",
    "                       2 * self.occupancy_grid_size + 1, 1)\n",
    "        elif feature == \"perimeter\":\n",
    "            shape_1 = (len(index_array), self.min_track_length, 1)\n",
    "            shape_2 = (len(index_array), 1, 1)\n",
    "        else:\n",
    "            raise ValueError(\"_compute_feature_shape: Unknown feature '{}'\".format(feature))\n",
    "        \n",
    "        return shape_1, shape_2\n",
    "    \n",
    "    def _compute_feature(self, feature, *args, **kwargs):\n",
    "        if feature == \"appearance\":\n",
    "            return self._compute_appearances(*args, **kwargs)\n",
    "        elif feature == \"distance\":\n",
    "            return self._compute_distances(*args, **kwargs)\n",
    "        elif feature == \"neighborhood\":\n",
    "            return self._compute_occupancy_grids(*args, **kwargs)\n",
    "        elif feature == \"perimeter\":\n",
    "            return self._compute_perimeters(*args, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"_compute_feature: Unknown feature '{}'\".format(feature))\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        # Initialize batch_x_1, batch_x_2, and batch_y, as well as cell distance data\n",
    "        # DVV Notes - I'm changing how this works. We will now only compare cells in neighboring\n",
    "        # frames. I am also modifying it so it will select a sequence of cells/distances for x1\n",
    "        # and 1 cell/distance for x2\n",
    "        \n",
    "        # setup zeroed batch arrays for each feature & batch_y\n",
    "        batch_features = []\n",
    "        for feature in self.features:\n",
    "            shape_1, shape_2 = self._compute_feature_shape(feature, index_array)\n",
    "            batch_features.append([np.zeros(shape_1, dtype=K.floatx()),\n",
    "                                   np.zeros(shape_2, dtype=K.floatx())])\n",
    "\n",
    "        batch_y = np.zeros((len(index_array), 3), dtype=np.int32)\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            # Identify which tracks are going to be selected\n",
    "            track_id = self.track_ids[j]\n",
    "            batch = track_id['batch']\n",
    "            label_1 = track_id['label']  \n",
    "            \n",
    "            X = self.x[batch]\n",
    "            y = self.y[batch]\n",
    "\n",
    "            # Choose comparison cell\n",
    "            # Determine what class the track will be - different (0), same (1), division (2)\n",
    "            division = False\n",
    "            type_cell = np.random.choice([0, 1, 2], p=[1/3, 1/3, 1/3])\n",
    "            #type_cell = np.random.choice([0, 1, 2], p=[1/2, 1/2, 0/3])\n",
    "            #type_cell = np.random.choice([0, 1, 2], p=[1/2, 0/3, 1/2])\n",
    "\n",
    "            # Dealing with edge cases\n",
    "            # If class is division, check if the first cell divides. If not, change tracks\n",
    "            if type_cell == 2:\n",
    "                division == True\n",
    "                if len(track_id['daughters']) == 0:\n",
    "                    # No divisions so randomly choose a different track that is\n",
    "                    # guaranteed to have a division\n",
    "                    new_j = np.random.choice(self.tracks_with_divisions)\n",
    "                    j = new_j\n",
    "                    track_id = self.track_ids[j]\n",
    "                    batch = track_id['batch']\n",
    "                    label_1 = track_id['label']\n",
    "                    X = self.x[batch]\n",
    "                    y = self.y[batch]\n",
    "\n",
    "            # Get the frames for cell 1 and frames/label for cell 2\n",
    "            frames_1 = self._fetch_frames(j, division=division)\n",
    "            \n",
    "            # For frame_2, choose the next frame cell 1 appears in \n",
    "            last_frame_1 = np.amax(frames_1)            \n",
    "            frame_2 = np.amin( [x for x in track_id['frames'] if x > last_frame_1] )\n",
    "            frames_2 = [frame_2]\n",
    "\n",
    "            different_cells = track_id['different'][frame_2]\n",
    "                    \n",
    "            if type_cell == 0:\n",
    "                # If there are no different cells in the subsequent frame, we must choose \n",
    "                # the same cell\n",
    "                if len(different_cells) == 0:\n",
    "                    type_cell = 1\n",
    "                else:\n",
    "                    label_2 = np.random.choice(different_cells)\n",
    "\n",
    "            if type_cell == 1:\n",
    "                # If there is only 1 cell in frame_2, we can only choose the class to be same\n",
    "                label_2 = label_1\n",
    "                        \n",
    "            if type_cell == 2:\n",
    "                # There should always be 2 daughters but not always a valid label\n",
    "                label_2 = np.int(np.random.choice(track_id['daughters'])) \n",
    "                daughter_track = self.reverse_track_ids[batch][label_2]\n",
    "                frame_2 = np.amin(self.track_ids[daughter_track]['frames'])\n",
    "                frames_2 = [frame_2]\n",
    "\n",
    "            track_1 = j\n",
    "            track_2 = self.reverse_track_ids[batch][label_2]\n",
    "\n",
    "            # compute desired features & save them to the batch arrays\n",
    "            if self.sync_transform:\n",
    "                # random angle & flips\n",
    "                transform = {\"theta\": 360 * np.random.random(),\n",
    "                             \"flip_horizontal\": np.random.random() < 0.5,\n",
    "                             \"flip_vertical\": np.random.random() < 0.5}\n",
    "            else:\n",
    "                transform = None\n",
    "            \n",
    "            for feature_i, feature in enumerate(self.features):\n",
    "                feature_1, feature_2 = self._compute_feature(feature,\n",
    "                                                             track_1, frames_1,\n",
    "                                                             track_2, frames_2,\n",
    "                                                             transform=transform)\n",
    "                batch_features[feature_i][0][i] = feature_1\n",
    "                batch_features[feature_i][1][i] = feature_2\n",
    "\n",
    "            batch_y[i, type_cell] = 1\n",
    "   \n",
    "        # prepare final batch list\n",
    "        batch_list = []\n",
    "        for feature_i, feature in enumerate(self.features):\n",
    "            batch_feature_1, batch_feature_2 = batch_features[feature_i]\n",
    "            # Remove singleton dimensions (if min_track_length is 1)\n",
    "            if self.squeeze:\n",
    "                if feature == \"appearance\":\n",
    "                    batch_feature_1 = np.squeeze(batch_feature_1, axis=self.time_axis)\n",
    "                    batch_feature_2 = np.squeeze(batch_feature_2, axis=self.time_axis)\n",
    "                else:\n",
    "                    batch_feature_1 = np.squeeze(batch_feature_1, axis=1)\n",
    "                    batch_feature_2 = np.squeeze(batch_feature_2, axis=1)\n",
    "\n",
    "            batch_list.append(batch_feature_1)\n",
    "            batch_list.append(batch_feature_2)\n",
    "\n",
    "        return batch_list, batch_y\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        Returns the next batch.\n",
    "        \"\"\"\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict, val_dict = get_data('/data/npz_data/cells/HeLa/S3/movie/nuclear_movie_hela0-7_same.npz',\n",
    "                                        mode='siamese_daughters')\n",
    "\n",
    "print('X_train shape:', train_dict['X'].shape)\n",
    "print('daughter shape: ', train_dict['daughters'].shape)\n",
    "print('daughter shape: ', train_dict['daughters'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_data_generator = SiameseDataGenerator(\n",
    "        rotation_range=180, # randomly rotate images by 0 to rotation_range degrees\n",
    "        shear_range=0,      # randomly shear images in the range (radians , -shear_range to shear_range)\n",
    "        horizontal_flip=0,  # randomly flip images\n",
    "        vertical_flip=0)\n",
    "\n",
    "test_iterator = SiameseIterator(train_dict,\n",
    "                                image_data_generator,\n",
    "                                occupancy_grid_size=40,\n",
    "                                crop_dim=32,\n",
    "                                min_track_length=5,\n",
    "                                features={\"appearance\", \"distance\", \"neighborhood\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imshow\n",
    "\n",
    "# Compare 2 images\n",
    "img_1 = train_dict['X'][1,0,:,:,0]\n",
    "img_2 = train_dict['X'][0,1,:,:,0]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,12))\n",
    "ax[0].imshow(img_1, interpolation='none', cmap='gray')\n",
    "ax[1].imshow(img_2, interpolation='none', cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(lst, y) = next(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lst))\n",
    "print(lst[0].shape)\n",
    "print(lst[1].shape)\n",
    "print(lst[2].shape)\n",
    "print(lst[3].shape)\n",
    "print(lst[4].shape)\n",
    "print(lst[5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = lst[0][4,4,:,:,0]\n",
    "img_2 = lst[1][4,0,:,:,0]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,12))\n",
    "ax[0].imshow(img_1, interpolation='none', cmap='gray')\n",
    "ax[1].imshow(img_2, interpolation='none', cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "for i in range(32):\n",
    "    if y[i,2] == 1:\n",
    "        plt.imshow(lst[4][i,0,:,:,0])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Distance (Centroid)  Data\n",
    "print(lst[2][3,:])\n",
    "plt.imshow(lst[4][18,4,:,:,0])\n",
    "print(np.round(lst[4][0,4,10,10,0], decimals=2))\n",
    "\n",
    "img = lst[4][5,:,:,:,:]\n",
    "img_old = lst[4][5,:,:,:,:]\n",
    "\n",
    "img_new = np.zeros(img.shape)\n",
    "datagen = ImageDataGenerator(rotation_range=30)\n",
    "for frame in range(img.shape[0]):\n",
    "    img_frame = img[frame]\n",
    "    print(img_frame.shape)\n",
    "    img_new[frame] = datagen.random_transform(img_frame)\n",
    "\n",
    "plt.imshow(img_old[0,:,:,0])\n",
    "plt.show()\n",
    "plt.imshow(img_new[0,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the labels\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements for model_zoo.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.activations import softmax\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Conv2D, Conv3D, ConvLSTM2D, LSTM\n",
    "from tensorflow.python.keras.layers import Add, Input, Concatenate, Lambda, InputLayer\n",
    "from tensorflow.python.keras.layers import MaxPool2D, MaxPool3D, AvgPool2D, UpSampling2D\n",
    "from tensorflow.python.keras.layers import Flatten, Dense, Dropout, Reshape\n",
    "from tensorflow.python.keras.layers import Activation, Softmax\n",
    "from tensorflow.python.keras.layers import BatchNormalization\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "\n",
    "from deepcell.layers import Resize\n",
    "from deepcell.layers import DilatedMaxPool2D, DilatedMaxPool3D\n",
    "from deepcell.layers import TensorProd2D, TensorProd3D\n",
    "from deepcell.layers import Location, Location3D\n",
    "from deepcell.layers import ImageNormalization2D, ImageNormalization3D\n",
    "\n",
    "#Siamese Model\n",
    "def siamese_model(\n",
    "    input_shape=None,\n",
    "    track_length=1,\n",
    "    features=None,\n",
    "    occupancy_grid_size=10,\n",
    "    reg=1e-5, init='he_normal',\n",
    "    softmax=True,\n",
    "    norm_method='std',\n",
    "    filter_size=61):\n",
    "    \n",
    "    def compute_input_shape(feature):\n",
    "        if feature == \"appearance\":\n",
    "            return input_shape\n",
    "        elif feature == \"distance\":\n",
    "            return (None, 2)\n",
    "        elif feature == \"neighborhood\":\n",
    "            return (None, 2 * occupancy_grid_size + 1, 2 * occupancy_grid_size + 1, 1)\n",
    "        elif feature == \"perimeter\":\n",
    "            return (None, 1)\n",
    "        else:\n",
    "            raise ValueError(\"samese_model.compute_input_shape: Unknown feature '{}'\".format(feature))\n",
    "            \n",
    "    def compute_reshape(feature):\n",
    "        if feature == \"appearance\":\n",
    "            return (64,)\n",
    "        elif feature == \"distance\":\n",
    "            return (2,)\n",
    "        elif feature == \"neighborhood\":\n",
    "            return (64,)\n",
    "        elif feature == \"perimeter\":\n",
    "            return (1,)\n",
    "        else:\n",
    "            raise ValueError(\"samese_model.compute_output_shape: Unknown feature '{}'\".format(feature))\n",
    "    \n",
    "    def compute_feature_extractor(feature, shape):\n",
    "        if feature == \"appearance\":\n",
    "            # This should not stay: channels_first/last should be used to dictate size (1 works for either right now)\n",
    "            N_layers = np.int(np.floor(np.log2(input_shape[1])))\n",
    "            feature_extractor = Sequential()\n",
    "            feature_extractor.add(InputLayer(input_shape=shape))\n",
    "            for layer in range(N_layers):\n",
    "                feature_extractor.add(Conv3D(64, (1, 3, 3),\n",
    "                                             kernel_initializer=init,\n",
    "                                             padding='same', \n",
    "                                             kernel_regularizer=l2(reg)))\n",
    "                feature_extractor.add(BatchNormalization(axis=channel_axis))\n",
    "                feature_extractor.add(Activation('relu'))\n",
    "                feature_extractor.add(MaxPool3D(pool_size=(1, 2, 2)))\n",
    "\n",
    "            feature_extractor.add(Reshape((-1, 64)))\n",
    "            return feature_extractor\n",
    "    \n",
    "        elif feature == \"distance\":\n",
    "            return None\n",
    "        elif feature == \"neighborhood\":\n",
    "            N_layers_og = np.int(np.floor(np.log2(2 * occupancy_grid_size + 1)))\n",
    "            feature_extractor_occupancy_grid = Sequential()\n",
    "            feature_extractor_occupancy_grid.add(\n",
    "                InputLayer(input_shape=(None, 2 * occupancy_grid_size + 1, 2 * occupancy_grid_size + 1, 1))\n",
    "            )   \n",
    "            for layer in range(N_layers_og):\n",
    "                feature_extractor_occupancy_grid.add(Conv3D(64, (1, 3, 3),\n",
    "                                                            kernel_initializer=init,\n",
    "                                                            padding='same', \n",
    "                                                            kernel_regularizer=l2(reg)))\n",
    "                feature_extractor_occupancy_grid.add(BatchNormalization(axis=channel_axis))\n",
    "                feature_extractor_occupancy_grid.add(Activation('relu'))\n",
    "                feature_extractor_occupancy_grid.add(MaxPool3D(pool_size=(1, 2, 2)))\n",
    "\n",
    "            feature_extractor_occupancy_grid.add(Reshape((-1, 64)))\n",
    "        \n",
    "            return feature_extractor_occupancy_grid\n",
    "        elif feature == \"perimeter\":\n",
    "            return None\n",
    "        else:\n",
    "            raise ValueError(\"samese_model.compute_feature_extractor: Unknown feature '{}'\".format(feature))\n",
    "\n",
    "    if features is None:\n",
    "        raise ValueError(\"siamese_model: No features specified.\")\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "        input_shape = (input_shape[0], None, *input_shape[1:])\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "        input_shape = (None, *input_shape)\n",
    "    \n",
    "    features = sorted(features)\n",
    "    \n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for feature in features:\n",
    "        in_shape = compute_input_shape(feature)\n",
    "        re_shape = compute_reshape(feature)\n",
    "        feature_extractor = compute_feature_extractor(feature, in_shape)\n",
    "        \n",
    "        layer_1 = Input(shape=in_shape)\n",
    "        layer_2 = Input(shape=in_shape)\n",
    "        \n",
    "        inputs.extend([layer_1, layer_2])\n",
    "        \n",
    "        # apply feature_extractor if it exists\n",
    "        if feature_extractor is not None:\n",
    "            layer_1 = feature_extractor(layer_1)\n",
    "            layer_2 = feature_extractor(layer_2)\n",
    "        \n",
    "        # LSTM on 'left' side of network since that side takes in stacks of features\n",
    "        layer_1 = LSTM(64)(layer_1)\n",
    "        layer_2 = Reshape(re_shape)(layer_2)\n",
    "        \n",
    "        outputs.append([layer_1, layer_2])\n",
    "\n",
    "    dense_merged = []\n",
    "    for layer_1, layer_2 in outputs:\n",
    "        merge = Concatenate(axis=channel_axis)([layer_1, layer_2])\n",
    "        dense_merge = Dense(128)(merge)\n",
    "        bn_merge = BatchNormalization(axis=channel_axis)(dense_merge)\n",
    "        dense_relu = Activation('relu')(bn_merge)\n",
    "        dense_merged.append(dense_relu)\n",
    "    \n",
    "    # Concatenate outputs from both instances\n",
    "    merged_outputs = Concatenate(axis=channel_axis)(dense_merged)\n",
    "\n",
    "    # Add dense layers\n",
    "    dense1 = Dense(128)(merged_outputs)\n",
    "    bn1 = BatchNormalization(axis=channel_axis)(dense1)\n",
    "    relu1 = Activation('relu')(bn1)\n",
    "    dense2 = Dense(128)(relu1)\n",
    "    bn2 = BatchNormalization(axis=channel_axis)(dense2)\n",
    "    relu2 = Activation('relu')(bn2)\n",
    "    dense3 = Dense(3, activation='softmax')(relu2)\n",
    "\n",
    "    # Instantiate model\n",
    "    final_layer = dense3\n",
    "    model = Model(inputs=inputs, outputs=final_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements for training.py\n",
    "from skimage.external import tifffile as tiff\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.utils import to_categorical as keras_to_categorical\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from deepcell import losses\n",
    "from deepcell.losses import weighted_categorical_crossentropy\n",
    "from deepcell.utils.io_utils import get_images_from_directory\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "from deepcell.utils.transform_utils import to_categorical\n",
    "from deepcell.settings import CHANNELS_FIRST\n",
    "\n",
    "# from deepcell import image_generators as generators\n",
    "\n",
    "# Training function\n",
    "\n",
    "def train_model_siamese_daughter(model=None,\n",
    "                                 dataset=None,\n",
    "                                 optimizer=None,\n",
    "                                 min_track_length=1,\n",
    "                                 features=None,\n",
    "                                 expt='',\n",
    "                                 it=0, batch_size=1, n_epoch=100,\n",
    "                                 direc_save='/data/models', direc_data='/data/npz_data',\n",
    "                                 focal=False,\n",
    "                                 gamma=0.5,\n",
    "                                 lr_sched=rate_scheduler(lr=0.01, decay=0.95),\n",
    "                                 rotation_range=0, flip=True, shear=0, class_weight=None):\n",
    "    \n",
    "    is_channels_first = K.image_data_format() == 'channels_first'\n",
    "    training_data_file_name = os.path.join(direc_data, dataset + '.npz')\n",
    "    todays_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    features_fmt = '[' + ','.join(f[0] for f in sorted(features)) + ']'\n",
    "    features_fmt += '_n_epoch={}'.format(n_epoch)\n",
    "    \n",
    "    file_name_save = os.path.join(direc_save, '{}_{}_{}_{}_{}.h5'.format(todays_date, dataset, features_fmt, expt, it))\n",
    "    file_name_save_loss = os.path.join(direc_save, '{}_{}_{}_{}_{}.npz'.format(todays_date, dataset, features_fmt, expt, it))\n",
    "\n",
    "    print(\"saving model at:\", file_name_save)\n",
    "    print(\"saving loss at:\", file_name_save_loss)\n",
    "    \n",
    "    train_dict, val_dict = get_data(training_data_file_name, mode='siamese_daughters')\n",
    "    #train_dict, val_dict = get_data(training_data_file_name, mode='siamese_daughters', test_size=.2)\n",
    "\n",
    "    class_weights = train_dict['class_weights']\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    print('X_train shape:', train_dict['X'].shape)\n",
    "    print('y_train shape:', train_dict['y'].shape)\n",
    "    print('X_test shape:', val_dict['X'].shape)\n",
    "    print('y_test shape:', val_dict['y'].shape)\n",
    "    print('Output Shape:', model.layers[-1].output_shape)\n",
    "\n",
    "    n_classes = model.layers[-1].output_shape[1 if is_channels_first else -1]\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        if focal:\n",
    "            return losses.weighted_focal_loss(y_true, y_pred,\n",
    "                                              gamma=gamma,\n",
    "                                              n_classes=n_classes,\n",
    "                                              from_logits=False)\n",
    "        else:\n",
    "            return losses.weighted_categorical_crossentropy(y_true, y_pred,\n",
    "                                                            n_classes=n_classes,\n",
    "                                                            from_logits=False)\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    print('Using real-time data augmentation.')\n",
    "\n",
    "    # this will do preprocessing and realtime data augmentation\n",
    "    datagen = SiameseDataGenerator(\n",
    "        rotation_range=rotation_range,  # randomly rotate images by 0 to rotation_range degrees\n",
    "        shear_range=shear,  # randomly shear images in the range (radians , -shear_range to shear_range)\n",
    "        horizontal_flip=flip,  # randomly flip images\n",
    "        vertical_flip=flip)  # randomly flip images\n",
    "\n",
    "    datagen_val = SiameseDataGenerator(\n",
    "        rotation_range=0,  # randomly rotate images by 0 to rotation_range degrees\n",
    "        shear_range=0,  # randomly shear images in the range (radians , -shear_range to shear_range)\n",
    "        horizontal_flip=0,  # randomly flip images\n",
    "        vertical_flip=0)  # randomly flip images\n",
    "\n",
    "    def count_pairs(y):\n",
    "        \"\"\"\n",
    "        Compute number of training samples needed to (stastically speaking)\n",
    "        observe all cell pairs.\n",
    "        Assume that the number of images is encoded in the second dimension.\n",
    "        Assume that y values are a cell-uniquely-labeled mask.\n",
    "        Assume that a cell is paired with one of its other frames 50% of the time\n",
    "        and a frame from another cell 50% of the time.\n",
    "        \"\"\"\n",
    "        # TODO: channels_first axes\n",
    "        total_pairs = 0\n",
    "        for image_set in range(y.shape[0]):\n",
    "            set_cells = 0\n",
    "            cells_per_image = []\n",
    "            for image in range(y.shape[1]):\n",
    "                image_cells = int(y[image_set, image, :, :, :].max())\n",
    "                set_cells = set_cells + image_cells\n",
    "                cells_per_image.append(image_cells)\n",
    "\n",
    "            # Since there are many more possible non-self pairings than there are self pairings,\n",
    "            # we want to estimate the number of possible non-self pairings and then multiply\n",
    "            # that number by two, since the odds of getting a non-self pairing are 50%, to\n",
    "            # find out how many pairs we would need to sample to (statistically speaking)\n",
    "            # observe all possible cell-frame pairs.\n",
    "            # We're going to assume that the average cell is present in every frame. This will\n",
    "            # lead to an underestimate of the number of possible non-self pairings, but it's\n",
    "            # unclear how significant the underestimate is.\n",
    "            average_cells_per_frame = int(sum(cells_per_image) / len(cells_per_image))\n",
    "            non_self_cellframes = (average_cells_per_frame - 1) * len(cells_per_image)\n",
    "            non_self_pairings = non_self_cellframes * max(cells_per_image)\n",
    "            cell_pairings = non_self_pairings * 2\n",
    "            total_pairs = total_pairs + cell_pairings\n",
    "        return total_pairs\n",
    "\n",
    "    # This shouldn't remain long term.\n",
    "    total_train_pairs = count_pairs(train_dict['y'])\n",
    "    total_test_pairs = count_pairs(val_dict['y'])\n",
    "\n",
    "    print(\"total_train_pairs:\", total_train_pairs)\n",
    "    print(\"total_test_pairs:\", total_test_pairs)\n",
    "    print(\"batch size: \", batch_size)\n",
    "    print(\"validation_steps: \", total_test_pairs // batch_size)\n",
    "\n",
    "    # fit the model on the batches generated by datagen.flow()\n",
    "    loss_history = model.fit_generator(\n",
    "        datagen.flow(train_dict,\n",
    "                     batch_size=batch_size,\n",
    "                     min_track_length=min_track_length,\n",
    "                     features=features),\n",
    "        steps_per_epoch=total_train_pairs // batch_size,\n",
    "        epochs=n_epoch,\n",
    "        validation_data=datagen_val.flow(val_dict,\n",
    "                                         batch_size=batch_size,\n",
    "                                         min_track_length=min_track_length,\n",
    "                                         features=features),\n",
    "        validation_steps=total_test_pairs // batch_size,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(file_name_save, monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "            LearningRateScheduler(lr_sched)\n",
    "        ])\n",
    "\n",
    "    model.save_weights(file_name_save)\n",
    "    np.savez(file_name_save_loss, loss_history=loss_history.history)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "\n",
    "from deepcell import rate_scheduler\n",
    "\n",
    "from deepcell.training import train_model_siamese_daughter\n",
    "from deepcell.model_zoo import siamese_model\n",
    "\n",
    "direc_data = '/data/npz_data/cells/HeLa/S3/movie/'\n",
    "dataset = 'nuclear_movie_hela0-7_same'\n",
    "\n",
    "#direc_data = '/data/npz_data/cells/3T3/NIH/'\n",
    "#dataset = 'nuclear_movie_3t3_set1_same'\n",
    "\n",
    "training_data = np.load('{}{}.npz'.format(direc_data, dataset))\n",
    "\n",
    "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "lr_sched = rate_scheduler(lr=0.001, decay=0.99)\n",
    "in_shape = (32, 32, 1)\n",
    "model = siamese_model(input_shape=in_shape, features={\"appearance\", \"distance\", \"neighborhood\", \"perimeter\"})\n",
    "\n",
    "tracking_model = train_model_siamese_daughter(model=model,\n",
    "                                              dataset=dataset,\n",
    "                                              optimizer=optimizer,\n",
    "                                              expt='transform_sync_no_diff',\n",
    "                                              it=0,\n",
    "                                              batch_size=128,\n",
    "                                              min_track_length=6,\n",
    "                                              features={\"appearance\", \"distance\", \"neighborhood\", \"perimeter\"},\n",
    "                                              n_epoch=5,\n",
    "                                              direc_save='/data/models/cells/HeLa/S3',\n",
    "                                              direc_data=direc_data,\n",
    "                                              lr_sched=lr_sched,\n",
    "                                              rotation_range=180,\n",
    "                                              flip=True,\n",
    "                                              shear=0,\n",
    "                                              class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/data/models/cells/HeLa/S3/2018-10-12_nuclear_movie_hela0-7_same_[a,d,n,p]_n_epoch=5_transform_sync_no_diff_0.npz\"\n",
    "loss_history = np.load(filename)\n",
    "loss_history[\"loss_history\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
