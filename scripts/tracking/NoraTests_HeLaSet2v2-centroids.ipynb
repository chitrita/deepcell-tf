{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From deepcell/utils/train_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import activations\n",
    "\n",
    "def axis_softmax(x, axis=1):\n",
    "    \n",
    "    return activations.softmax(x, axis=axis)\n",
    "\n",
    "def rate_scheduler(lr=.001, decay=0.95):\n",
    "    def output_fn(epoch):\n",
    "        epoch = np.int(epoch)\n",
    "        new_lr = lr * (decay ** epoch)\n",
    "        return new_lr\n",
    "    return output_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From deepcell/image_generators.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator, Iterator\n",
    "from tensorflow.python.keras import backend as K\n",
    "from skimage.measure import regionprops\n",
    "from skimage.transform import resize\n",
    "\n",
    "class SiameseDataGenerator(ImageDataGenerator):\n",
    "    def flow(self, train_dict, crop_dim=14, min_track_length=5,\n",
    "             batch_size=32, shuffle=True, seed=None, data_format=None,\n",
    "             save_to_dir=None, save_prefix='', save_format='png'):\n",
    "        return SiameseIterator(train_dict, self, crop_dim=crop_dim,\n",
    "                               min_track_length=min_track_length, batch_size=batch_size,\n",
    "                               shuffle=shuffle, seed=seed, data_format=data_format,\n",
    "                               save_to_dir=save_to_dir, save_prefix=save_prefix,\n",
    "                               save_format=save_format)\n",
    "\n",
    "class SiameseIterator(Iterator):\n",
    "    def __init__(self, train_dict, image_data_generator,\n",
    "                 crop_dim=14, min_track_length=5, batch_size=32, shuffle=False,\n",
    "                 seed=None, data_format=None, save_to_dir=None, save_prefix='',\n",
    "                 save_format='png'):\n",
    "        # Identify the channel axis so the code works regardless of what dimension\n",
    "        # we are using for channels - the data in the train_dict should be channels last\n",
    "        if data_format is None:\n",
    "            data_format = K.image_data_format()\n",
    "\n",
    "        if data_format == 'channels_first':\n",
    "            self.channel_axis = 1\n",
    "            self.row_axis = 3\n",
    "            self.col_axis = 4\n",
    "            self.time_axis = 2\n",
    "        elif data_format == 'channels_last':\n",
    "            self.channel_axis = 4\n",
    "            self.row_axis = 2\n",
    "            self.col_axis = 3\n",
    "            self.time_axis = 1\n",
    "        self.x = np.asarray(train_dict['X'], dtype=K.floatx())\n",
    "        self.y = np.int32(train_dict['y'])\n",
    "        self.crop_dim = crop_dim\n",
    "        self.min_track_length = min_track_length\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.data_format = data_format\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "\n",
    "        self.track_ids = self._get_track_ids()\n",
    "\n",
    "        super(SiameseIterator, self).__init__(len(self.track_ids), batch_size, shuffle, seed)\n",
    "\n",
    "    def _get_track_ids(self):\n",
    "        \"\"\"\n",
    "        This function builds the track id's. It returns a dictionary that\n",
    "        contains the batch number and label number of each each track.\n",
    "        Creates unique cell IDs, as cell labels are NOT unique across batches.\n",
    "        \"\"\"\n",
    "        track_counter = 0\n",
    "        track_ids = {}\n",
    "        for batch in range(self.y.shape[0]):\n",
    "            y_batch = self.y[batch]\n",
    "            num_cells = np.amax(y_batch)\n",
    "            for cell in range(1, num_cells + 1):\n",
    "                # count number of pixels cell occupies in each frame\n",
    "                y_true = np.sum(y_batch == cell, axis=(self.row_axis - 1, self.col_axis - 1))\n",
    "                # get indices of frames where cell is present\n",
    "                y_index = np.where(y_true > 0)[0]\n",
    "                if y_index.size > 0: # if cell is present at all\n",
    "                    start_frame = np.amin(y_index)\n",
    "                    stop_frame = np.amax(y_index)\n",
    "                    track_ids[track_counter] = {\n",
    "                        'batch': batch,\n",
    "                        'label': cell,\n",
    "                        'frames': y_index\n",
    "                    }\n",
    "                    track_counter += 1\n",
    "        return track_ids\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        # initialize batch_x_1, batch_x_2, and batch_y\n",
    "        if self.data_format == 'channels_first':\n",
    "            img_shape = (len(index_array), self.x.shape[self.channel_axis], self.crop_dim, self.crop_dim)\n",
    "        else:\n",
    "            img_shape = (len(index_array), self.crop_dim, self.crop_dim, self.x.shape[self.channel_axis])\n",
    "        \n",
    "        data_shape = (len(index_array), 2,)\n",
    "        batch_x_1 = np.zeros(img_shape, dtype=K.floatx())\n",
    "        batch_x_2 = np.zeros(img_shape, dtype=K.floatx())\n",
    "        centroid_1 = np.zeros(data_shape, dtype=K.floatx())\n",
    "        centroid_2 = np.zeros(data_shape, dtype=K.floatx())\n",
    "        batch_y = np.zeros((len(index_array), 2), dtype=np.int32)\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            # Identify which tracks are going to be selected\n",
    "            track_id = self.track_ids[j]\n",
    "            batch = track_id['batch']\n",
    "            label_1 = track_id['label']\n",
    "            tracked_frames = track_id['frames']\n",
    "            frame_1 = np.random.choice(tracked_frames) # Select a frame from the track\n",
    "\n",
    "            X = self.x[batch]\n",
    "            y = self.y[batch]\n",
    "\n",
    "            # Choose comparison cell\n",
    "            # Determine what class the track will be - different (0), same (1)\n",
    "            is_same_cell = np.random.random_integers(0, 1)\n",
    "\n",
    "\n",
    "            all_labels = np.delete(np.unique(y), 0) # all labels in y but 0 (background)\n",
    "            acceptable_labels = np.delete(all_labels, np.where(all_labels == label_1))\n",
    "            \n",
    "            if len(acceptable_labels) == 0:\n",
    "                is_same_cell = 1\n",
    "            \n",
    "            # Select another frame from the same track\n",
    "            if is_same_cell:\n",
    "                label_2 = label_1\n",
    "                frame_2 = np.random.choice(track_id['frames'])\n",
    "                \n",
    "            # Select another frame from a different track\n",
    "            if not is_same_cell:\n",
    "                # all_labels = np.arange(1, np.amax(y) + 1)\n",
    "                #all_labels = np.delete(np.unique(y), 0) # all labels in y but 0 (background)\n",
    "                #acceptable_labels = np.delete(all_labels, np.where(all_labels == label_1))\n",
    "                #print(\"acceptable_labels:\", acceptable_labels)\n",
    "                #print(\"al_length: \", len(acceptable_labels))\n",
    "                is_valid_label = False\n",
    "\n",
    "                while not is_valid_label:\n",
    "                    # get a random cell label from our acceptable list\n",
    "                    label_2 = np.random.choice(acceptable_labels)\n",
    "\n",
    "                    # count number of pixels cell occupies in each frame\n",
    "                    y_true = np.sum(y == label_2, axis=(\n",
    "                        self.row_axis - 1, self.col_axis - 1, self.channel_axis - 1))\n",
    "\n",
    "                    y_index = np.where(y_true > 0)[0] # get frames where cell is present\n",
    "                    is_valid_label = y_index.any() # label_2 is in a frame\n",
    "                    if not is_valid_label:\n",
    "                        # remove invalid label from list of acceptable labels\n",
    "                        acceptable_labels = np.delete(\n",
    "                            acceptable_labels, np.where(acceptable_labels == label_2))\n",
    "\n",
    "                frame_2 = np.random.choice(y_index) # get random frame with label_2\n",
    "\n",
    "            # Get appearances\n",
    "            frames = [frame_1, frame_2]\n",
    "            labels = [label_1, label_2]\n",
    "\n",
    "            appearances = self._get_appearances(X, y, frames, labels)\n",
    "\n",
    "            centroid_1[i] = np.array(appearances[1][0])\n",
    "            centroid_2[i] = np.array(appearances[1][1])\n",
    "            \n",
    "            #print(\"centroids: \", centroids, \"     shape: \", centroids.shape)\n",
    "            #print(\"type: \", type(centroids), \"    elem type: \", centroids[0].shape)\n",
    "            appearances = appearances[0]\n",
    "            #print(\"appearances type: \", type(appearances), \"     length: \", len(appearances))\n",
    "            \n",
    "\n",
    "            if self.data_format == 'channels_first':\n",
    "                appearances = [appearances[:, 0], appearances[:, 1]]\n",
    "            else:\n",
    "                appearances = [appearances[0], appearances[1]]\n",
    "\n",
    "            # Apply random transformations\n",
    "            for k, appearance in enumerate(appearances):\n",
    "                appearance = self.image_data_generator.random_transform(appearance)\n",
    "                appearance = self.image_data_generator.standardize(appearance)\n",
    "                appearances[k] = appearance\n",
    "\n",
    "            batch_x_1[i] = appearances[0]\n",
    "            batch_x_2[i] = appearances[1]\n",
    "            #print(\"input1 model: \", type(batch_x_1), \"    shape: \", batch_x_1.shape)\n",
    "            #print(\"centroid_1: \", type(centroid_1), \"     shape: \", centroid_1.shape)\n",
    "            \n",
    "            batch_y[i, is_same_cell] = 1\n",
    "\n",
    "        return ([batch_x_1, batch_x_2, centroid_1, centroid_2], batch_y)\n",
    "\n",
    "    def _get_appearances(self, X, y, frames, labels):\n",
    "        channel_axis = self.channel_axis - 1\n",
    "        if self.data_format == 'channels_first':\n",
    "            appearance_shape = (X.shape[channel_axis], len(frames), self.crop_dim, self.crop_dim)\n",
    "        else:\n",
    "            appearance_shape = (len(frames), self.crop_dim, self.crop_dim, X.shape[channel_axis])\n",
    "        appearances = np.zeros(appearance_shape, dtype=K.floatx())\n",
    "        centroids = []\n",
    "        for counter, (frame, cell_label) in enumerate(zip(frames, labels)):\n",
    "            # Get the bounding box\n",
    "            y_frame = y[frame] if self.data_format == 'channels_last' else y[:, frame]\n",
    "            props = regionprops(np.int32(y_frame == cell_label))\n",
    "            minr, minc, maxr, maxc = props[0].bbox\n",
    "            centroids.append(props[0].centroid)\n",
    "            # Extract images from bounding boxes\n",
    "            if self.data_format == 'channels_first':\n",
    "                appearance = X[:, frame, minr:maxr, minc:maxc]\n",
    "                resize_shape = (X.shape[channel_axis], self.crop_dim, self.crop_dim)\n",
    "            else:\n",
    "                appearance = X[frame, minr:maxr, minc:maxc, :]\n",
    "                resize_shape = (self.crop_dim, self.crop_dim, X.shape[channel_axis])\n",
    "\n",
    "            # Resize images from bounding box\n",
    "            max_value = np.amax([np.amax(appearance), np.absolute(np.amin(appearance))])\n",
    "            appearance /= max_value\n",
    "            appearance = resize(appearance, resize_shape)\n",
    "            appearance *= max_value\n",
    "            if self.data_format == 'channels_first':\n",
    "                appearances[:, counter] = appearance\n",
    "            else:\n",
    "                appearances[counter] = appearance\n",
    "\n",
    "        return [appearances, centroids]\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x.\n",
    "        # Returns the next batch.\n",
    "        \"\"\"\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from deepcell.utils import get_data\n",
    "\n",
    "data = np.load('/data/npz_data/cells/unspecified_nuclear_data/nuclear_movie/nuclear_movie_same.npz')\n",
    "train_dict, (X_test, y_test) = get_data('/data/npz_data/cells/unspecified_nuclear_data/nuclear_movie/nuclear_movie_same.npz',\n",
    "                                        mode='siamese')\n",
    "image_data_generator = SiameseDataGenerator(\n",
    "        rotation_range=0,  # randomly rotate images by 0 to rotation_range degrees\n",
    "        shear_range=0, # randomly shear images in the range (radians , -shear_range to shear_range)\n",
    "        horizontal_flip=0,  # randomly flip images\n",
    "        vertical_flip=0) \n",
    "\n",
    "test_iterator = SiameseIterator(train_dict, image_data_generator, crop_dim=32)\n",
    "#test_iterator._get_track_ids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:104: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(lst, y) = test_iterator.next()\n",
    "\n",
    "# = data[0]\n",
    "\n",
    "#centroids = data[1]\n",
    "\n",
    "#print(lst[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff52d587d68>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imshow\n",
    "plt.imshow(lst[0][1,:,:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff52d4597f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(lst[1][1,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 35.07     434.5     ]\n",
      " [ 29.32353  201.47058 ]\n",
      " [ 30.326733  73.84158 ]\n",
      " [ 29.336538 143.97116 ]\n",
      " [ 11.93007  298.3147  ]\n",
      " [ 32.0099   184.16832 ]\n",
      " [ 21.404581 324.8015  ]\n",
      " [ 29.362318 276.5797  ]\n",
      " [ 45.719696 377.9394  ]\n",
      " [ 78.65487  150.00885 ]\n",
      " [ 16.872341 392.7447  ]\n",
      " [ 21.92381    8.32381 ]\n",
      " [ 68.358696  93.815216]\n",
      " [ 47.642857  39.86607 ]\n",
      " [ 46.873016 181.56349 ]\n",
      " [ 53.502514 322.27637 ]\n",
      " [ 35.689655 471.12643 ]\n",
      " [ 54.70297   83.14851 ]\n",
      " [ 55.453335 434.54666 ]\n",
      " [ 64.4321   289.7284  ]\n",
      " [ 64.68276  248.71724 ]\n",
      " [ 82.43137  346.35294 ]\n",
      " [ 44.88889   63.287037]\n",
      " [ 71.17757  408.49533 ]\n",
      " [ 93.662254 177.33775 ]\n",
      " [ 93.86364  283.12878 ]\n",
      " [ 95.871796 200.38461 ]\n",
      " [ 70.15      41.27    ]\n",
      " [ 89.21667   21.083334]\n",
      " [105.51376  159.85321 ]\n",
      " [ 86.297295 263.2072  ]\n",
      " [ 84.611115 423.99445 ]]\n"
     ]
    }
   ],
   "source": [
    "print(lst[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From deepcell/training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "def siamese_model(input_shape=None, batch_shape=None, reg=1e-5, init='he_normal', permute=False, softmax=True, norm_method='std', filter_size=61):\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    input_1 = Input(shape=input_shape)\n",
    "    input_2 = Input(shape=input_shape)\n",
    "    input_3 = Input(shape=(2, ))\n",
    "    input_4 = Input(shape=(2, ))\n",
    "    \n",
    "    # Sequential interface for siamese portion of model\n",
    "    feature_extractor = Sequential()\n",
    "    feature_extractor.add(Conv2D(64, (3, 3), kernel_initializer=init, padding='same', kernel_regularizer=l2(reg), input_shape=input_shape))\n",
    "    feature_extractor.add(BatchNormalization(axis=channel_axis))\n",
    "    feature_extractor.add(Activation('relu'))\n",
    "    feature_extractor.add(Conv2D(64, (3, 3), kernel_initializer=init, padding='same', kernel_regularizer=l2(reg)))\n",
    "    feature_extractor.add(BatchNormalization(axis=channel_axis))\n",
    "    feature_extractor.add(Activation('relu'))\n",
    "    feature_extractor.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    feature_extractor.add(Conv2D(64, (3, 3), kernel_initializer=init, padding='same', kernel_regularizer=l2(reg)))\n",
    "    feature_extractor.add(BatchNormalization(axis=channel_axis))\n",
    "    feature_extractor.add(Activation('relu'))\n",
    "    feature_extractor.add(Conv2D(64, (3, 3), kernel_initializer=init, padding='same', kernel_regularizer=l2(reg)))\n",
    "    feature_extractor.add(BatchNormalization(axis=channel_axis))\n",
    "    feature_extractor.add(Activation('relu'))\n",
    "    feature_extractor.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Create two instances of feature_extractor\n",
    "    output_1 = feature_extractor(input_1)\n",
    "    #merged_output1 = concatenate([output_1, input_3], axis=channel_axis)\n",
    "    ##gmp_1 = GlobalMaxPooling2D()(output1)\n",
    "    ##gmp_2 = GlobalMaxPooling2D()(input_3)\n",
    "    ##merged_output1 = concatenate([gmp_1, gmp_2])\n",
    "    \n",
    "    output_2 = feature_extractor(input_2)\n",
    "    #merged_output2 = concatenate([output_2, input_4], axis=channel_axis)\n",
    "\n",
    "    flat1 = Flatten()(output_1)\n",
    "    flat2 = Flatten()(output_2)\n",
    "    #flat3 = Flatten()(input_3)\n",
    "    #flat4 = Flatten()(input_4)\n",
    "    merge_1 = concatenate([flat1, input_3])\n",
    "    merge_2 = concatenate([flat2, input_4])\n",
    "    # Concatenate outputs from both feature_extractor instances\n",
    "    \"\"\"\n",
    "    merged_outputs = Concatenate(axis=channel_axis)([output_1, output_2])\n",
    "    flat1 = Flatten()(merged_outputs)\n",
    "    distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(input_3, input_4))))\n",
    "    print(distance, \"     type:\", type(distance))\n",
    "    merge_dist = concatenate([flat1, distance])\n",
    "    \"\"\"\n",
    "    merged_outputs = Concatenate(axis=channel_axis)([merge_1, merge_2])\n",
    "    # Implement dense net (or call preexisting one?) with the two outputs as inputs\n",
    "    #dense1 = Dense(128)(merge_dist)\n",
    "    dense1 = Dense(128)(merged_outputs)\n",
    "    bn1 = BatchNormalization(axis=channel_axis)(dense1)\n",
    "    relu1 = Activation('relu')(bn1)\n",
    "    dense2 = Dense(128)(relu1)\n",
    "    bn2 = BatchNormalization(axis=channel_axis)(dense2)\n",
    "    relu2 = Activation('relu')(bn2)\n",
    "    dense3 = Dense( 2, activation='softmax')(relu2)\n",
    "\n",
    "    # Instantiate model\n",
    "    final_layer = dense3\n",
    "    model = Model(inputs=[input_1, input_2, input_3, input_4], outputs=final_layer)\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.losses import categorical_crossentropy, weighted_categorical_crossentropy\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "def train_model_siamese(model=None, dataset=None, optimizer=None,\n",
    "                        expt='', it=0, batch_size=1, n_epoch=100,\n",
    "                        direc_save='/data/models', direc_data='/data/npz_data',\n",
    "                        lr_sched=rate_scheduler(lr=0.01, decay=0.95),\n",
    "                        rotation_range=0, flip=True, shear=0, class_weight=None):\n",
    "    CHANNELS_FIRST = False\n",
    "    training_data_file_name = os.path.join(direc_data, dataset + '.npz')\n",
    "    todays_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    file_name_save = os.path.join(direc_save, '{}_{}_{}_{}.h5'.format(todays_date, dataset, expt, it))\n",
    "    file_name_save_loss = os.path.join(direc_save, '{}_{}_{}_{}.npz'.format(todays_date, dataset, expt, it))\n",
    "\n",
    "    train_dict, (X_test, y_test) = get_data(training_data_file_name, mode='siamese')\n",
    "\n",
    "    class_weights = train_dict['class_weights']\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    print('X_train shape:', train_dict['X'].shape)\n",
    "    print('y_train shape:', train_dict['y'].shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "    print('y_test shape:', y_test.shape)\n",
    "    print('Output Shape:', model.layers[-1].output_shape)\n",
    "\n",
    "    n_classes = model.layers[-1].output_shape[1 if CHANNELS_FIRST else -1]\n",
    "\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    def loss_function(y_true, y_pred):\n",
    "        return weighted_categorical_crossentropy(y_true, y_pred,\n",
    "                                                 n_classes=n_classes,\n",
    "                                                 from_logits=False)\n",
    "    \n",
    "    model.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    print('Using real-time data augmentation.')\n",
    "\n",
    "    # this will do preprocessing and realtime data augmentation\n",
    "    datagen = SiameseDataGenerator(\n",
    "        rotation_range=rotation_range,  # randomly rotate images by 0 to rotation_range degrees\n",
    "        shear_range=shear, # randomly shear images in the range (radians , -shear_range to shear_range)\n",
    "        horizontal_flip=flip,  # randomly flip images\n",
    "        vertical_flip=flip)  # randomly flip images\n",
    "\n",
    "    datagen_val = SiameseDataGenerator(\n",
    "        rotation_range=0,  # randomly rotate images by 0 to rotation_range degrees\n",
    "        shear_range=0, # randomly shear images in the range (radians , -shear_range to shear_range)\n",
    "        horizontal_flip=0,  # randomly flip images\n",
    "        vertical_flip=0)  # randomly flip images\n",
    "\n",
    "    validation_dict = {'X': X_test, 'y': y_test}\n",
    "\n",
    "    def count_pairs(y):\n",
    "        \"\"\"\n",
    "        Compute number of training samples needed to (stastically speaking)\n",
    "        observe all cell pairs.\n",
    "        Assume that the number of images is encoded in the second dimension.\n",
    "        Assume that y values are a cell-uniquely-labeled mask.\n",
    "        Assume that a cell is paired with one of its other frames 50% of the time\n",
    "        and a frame from another cell 50% of the time.\n",
    "        \"\"\"\n",
    "        # TODO: channels_first axes\n",
    "        total_pairs = 0\n",
    "        for image_set in range(y.shape[0]):\n",
    "            set_cells = 0\n",
    "            cells_per_image = []\n",
    "            for image in range(y.shape[1]):\n",
    "                image_cells = int(y[image_set, image, :, :, :].max())\n",
    "                set_cells = set_cells + image_cells\n",
    "                cells_per_image.append(image_cells)\n",
    "\n",
    "            # Since there are many more possible non-self pairings than there are self pairings,\n",
    "            # we want to estimate the number of possible non-self pairings and then multiply\n",
    "            # that number by two, since the odds of getting a non-self pairing are 50%, to\n",
    "            # find out how many pairs we would need to sample to (statistically speaking)\n",
    "            # observe all possible cell-frame pairs.\n",
    "            # We're going to assume that the average cell is present in every frame. This will\n",
    "            # lead to an underestimate of the number of possible non-self pairings, but it's\n",
    "            # unclear how significant the underestimate is.\n",
    "            average_cells_per_frame = int(sum(cells_per_image) / len(cells_per_image))\n",
    "            non_self_cellframes = (average_cells_per_frame - 1) * len(cells_per_image)\n",
    "            non_self_pairings = non_self_cellframes * max(cells_per_image)\n",
    "            cell_pairings = non_self_pairings * 2\n",
    "            total_pairs = total_pairs + cell_pairings\n",
    "        return total_pairs\n",
    "\n",
    "    # This shouldn't remain long term.\n",
    "    #magic_number = 2048  # A power of 2 chosen just to reduce training time.\n",
    "    total_train_pairs = count_pairs(train_dict['y'])\n",
    "    print(\"total_train_pairs:\", total_train_pairs)\n",
    "    #total_train_pairs = int(total_train_pairs // magic_number)\n",
    "    #print(\"total_train_pairs:\", total_train_pairs)\n",
    "\n",
    "    total_test_pairs = count_pairs(y_test)\n",
    "    #print(\"total_test_pairs:\", total_test_pairs)\n",
    "    #total_test_pairs = int(total_test_pairs // magic_number)\n",
    "    print(\"total_test_pairs:\", total_test_pairs)\n",
    "    print(\"batch size: \", batch_size)\n",
    "    print(\"validation_steps: \", total_test_pairs // batch_size)\n",
    "    test_1, test_2 = datagen.flow(train_dict, batch_size=batch_size, crop_dim=32).next()\n",
    "\n",
    "\n",
    "    \n",
    "    # fit the model on the batches generated by datagen.flow()\n",
    "    loss_history = model.fit_generator(\n",
    "        datagen.flow(train_dict, batch_size=batch_size, crop_dim=32),\n",
    "        steps_per_epoch=total_train_pairs // batch_size,\n",
    "        epochs=n_epoch,\n",
    "        validation_data=datagen_val.flow(validation_dict, batch_size=batch_size, crop_dim=32),\n",
    "        validation_steps=total_test_pairs // batch_size,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(file_name_save, monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "            LearningRateScheduler(lr_sched)\n",
    "        ])\n",
    "\n",
    "    model.save_weights(file_name_save)\n",
    "    np.savez(file_name_save_loss, loss_history=loss_history.history)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From deepcell/scripts/testing_scripts/nuclear_movie_generator_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 32, 32, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 32, 32, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 8, 8, 64)     112448      input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 4096)         0           sequential_4[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 4096)         0           sequential_4[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 4098)         0           flatten_7[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 4098)         0           flatten_8[0][0]                  \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 8196)         0           concatenate_10[0][0]             \n",
      "                                                                 concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          1049216     concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 128)          512         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 128)          0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          16512       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 128)          512         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 128)          0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 2)            258         activation_24[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,179,458\n",
      "Trainable params: 1,178,434\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "input_13\n",
      "input_14\n",
      "sequential_4\n",
      "flatten_7\n",
      "input_15\n",
      "flatten_8\n",
      "input_16\n",
      "concatenate_10\n",
      "concatenate_11\n",
      "concatenate_12\n",
      "dense_10\n",
      "batch_normalization_23\n",
      "activation_23\n",
      "dense_11\n",
      "batch_normalization_24\n",
      "activation_24\n",
      "dense_12\n",
      "X_train shape: (21, 40, 216, 256, 1)\n",
      "y_train shape: (21, 40, 216, 256, 1)\n",
      "X_test shape: (3, 40, 216, 256, 1)\n",
      "y_test shape: (3, 40, 216, 256, 1)\n",
      "Output Shape: (None, 2)\n",
      "Using real-time data augmentation.\n",
      "total_train_pairs: 191520\n",
      "total_test_pairs: 32880\n",
      "batch size:  128\n",
      "validation_steps:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:104: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1495/1496 [============================>.] - ETA: 4s - loss: 0.1401 - acc: 0.9514\n",
      "Epoch 00001: val_loss improved from inf to 0.63370, saving model to /data/models/cells/HeLa/S3/2018-07-23_nuclear_movie_HeLa2_raw_same__0.h5\n",
      "1496/1496 [==============================] - 6359s 4s/step - loss: 0.1401 - acc: 0.9514 - val_loss: 0.6337 - val_acc: 0.9284\n",
      "Epoch 2/10\n",
      "1495/1496 [============================>.] - ETA: 4s - loss: 0.0941 - acc: 0.9677\n",
      "Epoch 00002: val_loss improved from 0.63370 to 0.08736, saving model to /data/models/cells/HeLa/S3/2018-07-23_nuclear_movie_HeLa2_raw_same__0.h5\n",
      "1496/1496 [==============================] - 6360s 4s/step - loss: 0.0941 - acc: 0.9677 - val_loss: 0.0874 - val_acc: 0.9662\n",
      "Epoch 3/10\n",
      "1495/1496 [============================>.] - ETA: 3s - loss: 0.0795 - acc: 0.9733\n",
      "Epoch 00003: val_loss did not improve\n",
      "1496/1496 [==============================] - 6342s 4s/step - loss: 0.0795 - acc: 0.9733 - val_loss: 0.1321 - val_acc: 0.9582\n",
      "Epoch 4/10\n",
      "1495/1496 [============================>.] - ETA: 4s - loss: 0.0717 - acc: 0.9759\n",
      "Epoch 00004: val_loss improved from 0.08736 to 0.06639, saving model to /data/models/cells/HeLa/S3/2018-07-23_nuclear_movie_HeLa2_raw_same__0.h5\n",
      "1496/1496 [==============================] - 6390s 4s/step - loss: 0.0717 - acc: 0.9759 - val_loss: 0.0664 - val_acc: 0.9780\n",
      "Epoch 5/10\n",
      "1348/1496 [==========================>...] - ETA: 9:47 - loss: 0.0659 - acc: 0.9776"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import datetime\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.activations import softmax\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Add, Permute, Input, Concatenate, concatenate\n",
    "from tensorflow.python.keras.layers import Conv2D, Conv3D, MaxPool2D, AvgPool2D\n",
    "from tensorflow.python.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.python.keras.layers import Activation, Softmax\n",
    "from tensorflow.python.keras.layers import BatchNormalization\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "\n",
    "from deepcell.utils import get_data\n",
    "\n",
    "\n",
    "\n",
    "#direc_data = '/data/npz_data/cells/HeLa/S3/movie/'\n",
    "#dataset = 'nuclear_movie_hela2_raw_same'\n",
    "direc_data = '/data/npz_data/cells/HeLa/S3/set2/movie1/'\n",
    "dataset = 'nuclear_movie_HeLa2_raw_same'\n",
    "\n",
    "training_data = np.load('{}{}.npz'.format(direc_data, dataset))\n",
    "\n",
    "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "lr_sched = rate_scheduler(lr=0.01, decay=0.99)\n",
    "in_shape = (32, 32, 1)\n",
    "model = siamese_model(input_shape=in_shape)\n",
    "for layer in model.layers:\n",
    "    print(layer.name)\n",
    "\n",
    "train_model_siamese(model=model,\n",
    "                    dataset='nuclear_movie_HeLa2_raw_same',\n",
    "                    optimizer=optimizer,\n",
    "                    expt='',\n",
    "                    it=0,\n",
    "                    batch_size=128,\n",
    "                    n_epoch=10,\n",
    "                    direc_save='/data/models/cells/HeLa/S3',\n",
    "                    direc_data='/data/npz_data/cells/HeLa/S3/set2/movie1/',\n",
    "                    lr_sched=lr_sched,\n",
    "                    rotation_range=0,\n",
    "                    flip=True,\n",
    "                    shear=0,\n",
    "                    class_weight=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
